{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Is this email a spam?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many email services today provide spam filters that are able to classify emails into spam and non-spam email with high accuracy. In this article I would like to cover a simple approach to detect if an email is a spam or not building my own spam filter from scratch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first time I heard about this problem, I wondered my self how is possible to achieve this having in input a dataset of just text (the emails). Machine learning algorithms need to be fed with numbers, they do not understand strings. So we need to find a method to transform the dataset, which is basically composed by a sequence of strings, into a sequence of numbers.<br>\n",
    "Let's give a glance at one possible idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One possible approach to adopt is the following:\n",
    "\n",
    "  \n",
    "  1. first thing we need is a dataset containing the most frequently occurring spam words (here we call it **SD**, Spam Dataset); <br><br>For each email within our dataset we do the following steps: <br><br>\n",
    "  2. create a list of integer with all the values set to $0$ with the same length of *SD* (here we call this list **EL**, Email List);\n",
    "    \n",
    "  3. next, to fill the list of integer properly, we use the following idea:<br>\n",
    "        if the first spamming word cointained in *SD* is present in the email took into account, then we assign the value $1$ in the first cell of EL. <br>Continue with this approach considering all the spamming words in *SD*.\n",
    "    \n",
    "Doing so, we end up with a list of ones and zeros for each email.\n",
    "\n",
    "Maybe an example would be explanatory if you are a bit confusing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following email:\n",
    "    <center>\"<i>Artificial   intelligence   will   destroy   humanity!</i>\"</center>\n",
    "    \n",
    "Let's say our *SD* is composed by the following spam words (step 1):\n",
    "- artificial\n",
    "- dollars\n",
    "- cost\n",
    "- less\n",
    "- destroy\n",
    "- money\n",
    "- free\n",
    "\n",
    "Consequently our *EL* would be the following list (step 2):\n",
    "$$[0, 0, 0, 0, 0, 0, 0]$$\n",
    "\n",
    "After the third step, we end up with this list: \n",
    "$$[1, 0, 0, 0, 1, 0, 0]$$\n",
    "\n",
    "If we continue by doing this for each email within our dataset, we manage to create a dataset of numbers and we  finally have something to pass to a learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, here we load the `email.csv` dataset and take a look at the percentage of the spam and not-spam emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.rendered_html { font-size: 17px; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.rendered_html { font-size: 17px; }</style>\"))\n",
    "\n",
    "df = pd.read_csv('emails.csv')\n",
    "#df['spam'].value_counts().plot(kind = 'pie', explode = [0, 0.1], figsize = (6, 6), autopct = '%1.1f%%', shadow = True)\n",
    "#plt.ylabel(\"Spam vs not spam\")\n",
    "#plt.legend([\"Not spam\", \"Spam\"])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"article/spam-email/img/1.png\" alt=\"\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to convert each email into a numeric representation, I have created the following util functions. The most interesting one is the function `process_email` that performs useful string operation. To cite an example, if the email we are processing contains some numbers, those are replaced with the word *number* (line of code 30). <br>Moreover, often the emails contains html tags. Since these tags could be tricky to manage, we simply get rid of them (line of code 24)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reads the fixed vocabulary list in vocab.txt and returns a cell array of the words in vocabList\n",
    "def get_vocabList():\n",
    "    vocab = pd.read_table('vocab.txt', delim_whitespace=True, header=None)\n",
    "    vocab = vocab.iloc[:,-1].values.tolist()\n",
    "    return vocab\n",
    "       \n",
    "\n",
    "# preprocesses the body of an email and returns a list of indices of the words contained in the email\n",
    "def process_email(email, is_file):\n",
    "    \n",
    "    # read email from a file is `is_file` is True, otherwise the parameter email is directly passed as a string \n",
    "    if is_file == True:\n",
    "        with open(email, 'r', encoding='ISO-8859-1') as myfile:\n",
    "            processed_email = myfile.read()\n",
    "            original_email = processed_email\n",
    "    else:\n",
    "        processed_email = email\n",
    "        original_email = email\n",
    "            \n",
    "    vocabList = get_vocabList()\n",
    "    \n",
    "    \n",
    "    # set all character in lower case\n",
    "    processed_email = str(processed_email).lower()\n",
    "\n",
    "    # strip all HTML\n",
    "    processed_email = re.sub('<[^<]+?>', '', processed_email)\n",
    "\n",
    "    # replace any number into the string 'number'\n",
    "    processed_email = re.sub('[0-9]+', 'number ', processed_email) \n",
    "\n",
    "    # replace strings starting with http:// or https:// with the string 'httpaddr'\n",
    "    processed_email = re.sub('(http|https)://[^\\s]*', 'httpaddr ', processed_email) \n",
    "\n",
    "    # replace email with string 'emailaddr'\n",
    "    processed_email = re.sub('[^\\s]+@[^\\s]+', 'emailaddr ', processed_email) \n",
    "\n",
    "    # replace dollar sign ($) with string 'dollar'\n",
    "    processed_email = re.sub('[$]+', 'dollar ', processed_email) \n",
    "    \n",
    "    # delete non-alphanumeric characters\n",
    "    processed_email = re.sub('[^a-zA-Z0-9]', ' ', processed_email)\n",
    "    \n",
    "    word_indices = np.array([])\n",
    "    for word in processed_email.split():\n",
    "        ps = PorterStemmer()\n",
    "        # stem the word\n",
    "        word = ps.stem(word)\n",
    "        \n",
    "        # if the word is in vocabList, insert in word_indices array the word's index of vocabList \n",
    "        for index in range(len(vocabList)):\n",
    "            if vocabList[index] == word:\n",
    "                word_indices = np.append(word_indices, index)\n",
    "    return word_indices, original_email, vocabList, processed_email\n",
    "\n",
    "\n",
    "# takes in a word_indices vector and produces a feature vector from the word indices\n",
    "def email_feature(word_indices, vocabList):\n",
    "    x = np.zeros(len(vocabList))\n",
    "    for index in range(len(word_indices)):\n",
    "        x[int(word_indices[index])] = 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some of the spamming words contained in the Spam Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-3f592210ab51>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-3f592210ab51>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    vocab = get_vocabList()\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "#spam_wordcloud = WordCloud(width=600, height=400).generate(\" \".join(vocab))\n",
    "#plt.figure( figsize=(10,8), facecolor='k')\n",
    "#plt.imshow(spam_wordcloud)\n",
    "#plt.axis(\"off\")\n",
    "#plt.tight_layout(pad=0)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"article/spam-email/img/2.png\" alt=\"\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert emails into their numeric representation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to apply the idea described earlier with the aim to convert emails into their numeric representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "emails = pd.read_csv('emails.csv')\n",
    "vocab = get_vocabList()\n",
    "X = emails.iloc[:, 0]\n",
    "y = emails.iloc[:, 1]\n",
    "X_rec = np.zeros((X.shape[0], len(vocab)))\n",
    "\n",
    "# process each mail separately\n",
    "for m in range(len(X)):\n",
    "    word_indices, original_email, vocabList, processed_email = process_email(X[m], is_file=False)\n",
    "    feature = email_feature(word_indices, vocabList)\n",
    "    X_rec[m, :] = feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done! The training time has come! <br>I have decided to use two learning algorithms:\n",
    "- Logistic regression\n",
    "- Neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training using Logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training and testing the model I decided to split the dataset into a *trainig set* and a *test set* (70 - 30). Actually, splitting the dataset in just two chunks (train + test) is a bad practice. It should be splitted in three different dataset: train + validation + test set. But for the purpose of this article train + test is just fine since we are not going to tune any model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4010, 1899), (4010,), (1718, 1899), (1718,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset_size = 1718\n",
    "X_rec_train = X_rec[:(X_rec.shape[0] - testset_size)]\n",
    "y_train = y[:(y.shape[0] - testset_size)]\n",
    "\n",
    "X_rec_test = X_rec[(X_rec.shape[0] - testset_size):]\n",
    "y_test = y[(X_rec.shape[0] - testset_size):]\n",
    "X_rec_train.shape, y_train.shape, X_rec_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs')\n",
    "m = clf.fit(X_rec_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model using test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accurcay: 98.72 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accurcay: {0:.2f}\".format((m.predict(X_rec_test) == y_test).sum()/X_rec_test.shape[0]*100), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An accuracy of 98.72% is formidable in spite of we did not tune any parameter! Let's see if a neural network can do better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainig using Neural Network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "emails = pd.read_csv('emails.csv')\n",
    "testset_size = 1500\n",
    "X_rec_train = X_rec[:(X_rec.shape[0] - testset_size)]\n",
    "y_train = y[:(y.shape[0] - testset_size)]\n",
    "\n",
    "X_rec_test = X_rec[(X_rec.shape[0] - testset_size):]\n",
    "y_test = y[(X_rec.shape[0] - testset_size):]\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=1899, activation='relu'))\n",
    "model.add(Dense(1899, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4228/4228 [==============================] - 3s 630us/step - loss: 0.1509 - acc: 0.9368\n",
      "Epoch 2/10\n",
      "4228/4228 [==============================] - 3s 688us/step - loss: 0.0324 - acc: 0.9898\n",
      "Epoch 3/10\n",
      "4228/4228 [==============================] - 3s 729us/step - loss: 0.0091 - acc: 0.9979\n",
      "Epoch 4/10\n",
      "4228/4228 [==============================] - 3s 765us/step - loss: 0.0077 - acc: 0.9969\n",
      "Epoch 5/10\n",
      "4228/4228 [==============================] - 3s 779us/step - loss: 0.0083 - acc: 0.9972\n",
      "Epoch 6/10\n",
      "4228/4228 [==============================] - 3s 757us/step - loss: 0.0028 - acc: 0.9995\n",
      "Epoch 7/10\n",
      "4228/4228 [==============================] - 3s 732us/step - loss: 0.0021 - acc: 0.9993\n",
      "Epoch 8/10\n",
      "4228/4228 [==============================] - 3s 626us/step - loss: 0.0011 - acc: 0.9993\n",
      "Epoch 9/10\n",
      "4228/4228 [==============================] - 2s 517us/step - loss: 0.0019 - acc: 0.9993\n",
      "Epoch 10/10\n",
      "4228/4228 [==============================] - 2s 540us/step - loss: 0.0017 - acc: 0.9993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ffb65ad0390>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_rec_train, y_train, epochs=10, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model using test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 107us/step\n",
      "\n",
      "acc: 98.53%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_rec_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network does a good job as well with an accuracy of 98.53%. It can actually do better if we tune parameters and choose a better architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the spam classifier using my emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Now the cool part!` <br>\n",
    "Here we test the spam classifier we have just built with our emails! <br>\n",
    "(For those who want to test the model with their emails, just edit the file `my_email.txt` by pasting your email)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pptr/.conda/envs/fastai/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: read_table is deprecated, use read_csv instead, passing sep='\\t'.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "my_email = 'my_email.txt'\n",
    "word_indices, original_email, vocabList, processed_email = process_email(my_email, is_file=True)\n",
    "X = email_feature(word_indices, vocabList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The email I want to test is a spam email I recently received, let's see if the model is tough enough to recognize the spam! <br>\n",
    "The email is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How can you make time to earn your MBA?\\nStart with choosing a program that is as flexible as it is valuable, like the iMBA.\\nPatricia, a full-time working mother and iMBA student...\\n\"Getting my MBA makes me feel empowered. I don창\\x80\\x99t need to stop working, I don창\\x80\\x99t need to stop being a mother, I don창\\x80\\x99t need to stop having my life, and that is everything to me.창\\x80\\x9d\\nContinue Reading\\nUPCOMING WEBINAR\\n\\nThursday, June 20\\n\\nThis webinar will provide an overview of the iMBA program, student experience and interactive curriculum. It will also discuss admission requirements, and participants will have an opportunity to ask questions for the admissions panel.\\nRSVP Now\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(my_email, 'r', encoding='ISO-8859-1') as myfile:\n",
    "            processed_email = myfile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is email is:  Spam\n"
     ]
    }
   ],
   "source": [
    "print(\"This is email is: \", np.where(np.round(model.predict(X.reshape(-1,1).T)) == 1, \"Spam\", \"Not spam\")[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model recognizes correctly it as a spam, superb!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
